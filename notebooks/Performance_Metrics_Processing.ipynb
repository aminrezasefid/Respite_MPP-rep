{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b453978f",
   "metadata": {},
   "source": [
    "This notebook is used to calculate mean and standard deviation of performance metrics. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a69fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f233340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1109da",
   "metadata": {},
   "source": [
    "# Calculate mean and standard deviation for the performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf930d0",
   "metadata": {},
   "source": [
    "-  Note: use Performance_Metrics_Calculation.ipynb to generate grand performance dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5634a",
   "metadata": {},
   "source": [
    "## Benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a38d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the dataset details\n",
    "folder = 'benchmark'\n",
    "task_setting = 'benchmark'\n",
    "mol_props = ['BACE', 'BBBP', 'HIV', 'ESOL', 'FreeSolv',  'Lipop'] \n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30\n",
    "\n",
    "#specify model names\n",
    "model_names = ['RF', 'molbert', 'grover_base', 'grover_base_rdkit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80069039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "agg_perf_df = pd.DataFrame(columns=['mean_metric_score', 'std_metric_score', 'split_type', 'mol_prop', 'model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615e6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the grand perf df\n",
    "grand_perf_df = pd.read_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "\n",
    "for split_type in split_types:\n",
    "    for model_name in model_names:\n",
    "        perf_df = grand_perf_df.loc[(grand_perf_df['split_type']==split_type) & (grand_perf_df['model_name']==model_name)]        \n",
    "\n",
    "        #calculate the mean and std of each metric score\n",
    "        for mol_prop in mol_props:\n",
    "            #get tmp1 by mol_prop\n",
    "            tmp_1 = perf_df[perf_df['mol_prop'] == mol_prop]\n",
    "            \n",
    "            #get the metric_name\n",
    "            if mol_prop in ['BACE', 'BBBP', 'HIV']:\n",
    "                #cls\n",
    "                metric_names = [\"AUROC\", \"AUPRC\", \"Precision_PPV\", \"Precision_NPV\"]\n",
    "            elif mol_prop in ['ESOL', 'FreeSolv', 'Lipop']:\n",
    "                #reg\n",
    "                metric_names = ['RMSE', 'R2', 'Pearson_R', 'MAE']\n",
    "\n",
    "            # loop through metric names\n",
    "            for metric_name in metric_names:\n",
    "                tmp_2 = tmp_1[tmp_1['metric_name'] == metric_name]\n",
    "\n",
    "                values_to_add = {'metric_name': metric_name, 'mean_metric_score': tmp_2['metric_score'].mean(), 'std_metric_score': tmp_2['metric_score'].std(),\\\n",
    "                                 'split_type': split_type, 'mol_prop': mol_prop, 'model_name':model_name}\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "\n",
    "                #append new row\n",
    "                agg_perf_df = agg_perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0884633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_metric_score</th>\n",
       "      <th>std_metric_score</th>\n",
       "      <th>split_type</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>model_name</th>\n",
       "      <th>metric_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.758733</td>\n",
       "      <td>0.058147</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>ESOL</td>\n",
       "      <td>RF</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.602236</td>\n",
       "      <td>0.166422</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>FreeSolv</td>\n",
       "      <td>RF</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.655029</td>\n",
       "      <td>0.044809</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>Lipop</td>\n",
       "      <td>RF</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.763882</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>ESOL</td>\n",
       "      <td>molbert</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.275915</td>\n",
       "      <td>0.145668</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>FreeSolv</td>\n",
       "      <td>molbert</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_metric_score  std_metric_score split_type  mol_prop model_name  \\\n",
       "14           0.758733          0.058147   scaffold      ESOL         RF   \n",
       "18           0.602236          0.166422   scaffold  FreeSolv         RF   \n",
       "22           0.655029          0.044809   scaffold     Lipop         RF   \n",
       "38           0.763882          0.068974   scaffold      ESOL    molbert   \n",
       "42           0.275915          0.145668   scaffold  FreeSolv    molbert   \n",
       "\n",
       "   metric_name  \n",
       "14   Pearson_R  \n",
       "18   Pearson_R  \n",
       "22   Pearson_R  \n",
       "38   Pearson_R  \n",
       "42   Pearson_R  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_perf_df[agg_perf_df['metric_name']=='Pearson_R'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff92d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_perf_df.to_csv('../results/processed_performance/{folder}_agg_perf_df_{task}.csv'.format(task=task_setting, folder=folder), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371100dd",
   "metadata": {},
   "source": [
    "## Opioids datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef6c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the dataset details\n",
    "folder = 'opioids'\n",
    "task_setting = 'reg' # cutoff6, reg\n",
    "mol_props = ['MDR1', 'CYP3A4', 'CYP2D6', 'MOR', 'DOR', 'KOR'] \n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30\n",
    "\n",
    "#specify model names\n",
    "model_names = ['RF', 'molbert', 'grover_base', 'grover_base_rdkit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b99d291",
   "metadata": {},
   "source": [
    "-  overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ae68388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "agg_perf_df = pd.DataFrame(columns=['mean_metric_score', 'std_metric_score', 'split_type', 'mol_prop', 'model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0347bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the grand perf df\n",
    "grand_perf_df = pd.read_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "\n",
    "for split_type in split_types:\n",
    "    for model_name in model_names:\n",
    "        perf_df = grand_perf_df.loc[(grand_perf_df['split_type']==split_type) & (grand_perf_df['model_name']==model_name)]        \n",
    "\n",
    "        #calculate the mean and std of each metric score\n",
    "        for mol_prop in mol_props:\n",
    "            #get tmp1 by mol_prop\n",
    "            tmp_1 = perf_df[perf_df['mol_prop'] == mol_prop]\n",
    "            \n",
    "            # get metric names\n",
    "            if task_setting == 'cutoff6':\n",
    "                metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "            elif task_setting == 'reg':\n",
    "                metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "\n",
    "            #get the metric_name\n",
    "            for metric_name in metric_names:\n",
    "                tmp_2 = tmp_1[tmp_1['metric_name'] == metric_name]\n",
    "\n",
    "                values_to_add = {'metric_name': metric_name, 'mean_metric_score': tmp_2['metric_score'].mean(), 'std_metric_score': tmp_2['metric_score'].std(),\\\n",
    "                                 'split_type': split_type, 'mol_prop': mol_prop, 'model_name':model_name}\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "\n",
    "                #append new row\n",
    "                agg_perf_df = agg_perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3946238a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_metric_score</th>\n",
       "      <th>std_metric_score</th>\n",
       "      <th>split_type</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>model_name</th>\n",
       "      <th>metric_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.817701</td>\n",
       "      <td>0.322271</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.319866</td>\n",
       "      <td>0.212408</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>MAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.684338</td>\n",
       "      <td>0.159458</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>R2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.827725</td>\n",
       "      <td>0.104071</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.594176</td>\n",
       "      <td>0.213225</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>CYP3A4</td>\n",
       "      <td>RF</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_metric_score  std_metric_score split_type mol_prop model_name  \\\n",
       "0           1.817701          0.322271   scaffold     MDR1         RF   \n",
       "1           1.319866          0.212408   scaffold     MDR1         RF   \n",
       "2           0.684338          0.159458   scaffold     MDR1         RF   \n",
       "3           0.827725          0.104071   scaffold     MDR1         RF   \n",
       "4           1.594176          0.213225   scaffold   CYP3A4         RF   \n",
       "\n",
       "  metric_name  \n",
       "0        RMSE  \n",
       "1         MAE  \n",
       "2          R2  \n",
       "3   Pearson_R  \n",
       "4        RMSE  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_perf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f08d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_perf_df.to_csv('../results/processed_performance/{folder}_agg_perf_df_{task}.csv'.format(task=task_setting, folder=folder), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cac9dc",
   "metadata": {},
   "source": [
    "-  Ac molecules and non-AC molecules respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e865fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "agg_perf_df = pd.DataFrame(columns=['mean_metric_score', 'std_metric_score', 'split_type', 'mol_prop', 'model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42611bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AC_labels = ['AC', 'non-AC']\n",
    "\n",
    "for AC_label in AC_labels:\n",
    "\n",
    "    #read the grand perf df\n",
    "    grand_perf_df = pd.read_csv('../results/processed_performance/AC_{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "    #get the AC label portion\n",
    "    grand_perf_df = grand_perf_df[grand_perf_df['AC_label'] == AC_label]\n",
    "\n",
    "    for split_type in split_types:\n",
    "        for model_name in model_names:\n",
    "            perf_df = grand_perf_df.loc[(grand_perf_df['split_type']==split_type) & (grand_perf_df['model_name']==model_name)]        \n",
    "\n",
    "            #calculate the mean and std of each metric score\n",
    "            for mol_prop in mol_props:\n",
    "                #get tmp1 by mol_prop\n",
    "                tmp_1 = perf_df[perf_df['mol_prop'] == mol_prop]\n",
    "                # get metric names\n",
    "                if task_setting == 'cutoff6':\n",
    "                    metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "                elif task_setting == 'reg':\n",
    "                    metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "\n",
    "                #get the metric_name\n",
    "                for metric_name in metric_names:\n",
    "                    tmp_2 = tmp_1[tmp_1['metric_name'] == metric_name]\n",
    "\n",
    "                    values_to_add = {'metric_name': metric_name, 'mean_metric_score': tmp_2['metric_score'].mean(), 'std_metric_score': tmp_2['metric_score'].std(),\\\n",
    "                     'split_type': split_type, 'mol_prop': mol_prop, 'model_name':model_name, 'AC_label':AC_label}\n",
    "                    row_to_add = pd.Series(values_to_add)\n",
    "\n",
    "                    #append new row\n",
    "                    agg_perf_df = agg_perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9898af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_metric_score</th>\n",
       "      <th>std_metric_score</th>\n",
       "      <th>split_type</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>model_name</th>\n",
       "      <th>AC_label</th>\n",
       "      <th>metric_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.928461</td>\n",
       "      <td>0.540381</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>AC</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.379168</td>\n",
       "      <td>0.308204</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>AC</td>\n",
       "      <td>MAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.583105</td>\n",
       "      <td>0.290243</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>AC</td>\n",
       "      <td>R2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.780169</td>\n",
       "      <td>0.189152</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>RF</td>\n",
       "      <td>AC</td>\n",
       "      <td>Pearson_R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.890989</td>\n",
       "      <td>0.423392</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>CYP3A4</td>\n",
       "      <td>RF</td>\n",
       "      <td>AC</td>\n",
       "      <td>RMSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_metric_score  std_metric_score split_type mol_prop model_name  \\\n",
       "0           1.928461          0.540381   scaffold     MDR1         RF   \n",
       "1           1.379168          0.308204   scaffold     MDR1         RF   \n",
       "2           0.583105          0.290243   scaffold     MDR1         RF   \n",
       "3           0.780169          0.189152   scaffold     MDR1         RF   \n",
       "4           1.890989          0.423392   scaffold   CYP3A4         RF   \n",
       "\n",
       "  AC_label metric_name  \n",
       "0       AC        RMSE  \n",
       "1       AC         MAE  \n",
       "2       AC          R2  \n",
       "3       AC   Pearson_R  \n",
       "4       AC        RMSE  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_perf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6c95947",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_perf_df.to_csv('../results/processed_performance/AC_{folder}_agg_perf_df_{task}.csv'\\\n",
    "                   .format(task=task_setting, folder=folder), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a9c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
