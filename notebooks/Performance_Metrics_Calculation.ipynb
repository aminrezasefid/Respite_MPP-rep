{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b453978f",
   "metadata": {},
   "source": [
    "This notebook is used to calculate the performance metrics based on raw predictions. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5872fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f233340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f126a",
   "metadata": {},
   "source": [
    "-  a function for metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffaba7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calc(labels, preds, metric='RMSE'):\n",
    "    # convert the labels and preds to list\n",
    "    labels = list(labels)\n",
    "    preds = list(preds)\n",
    "    # calculate score\n",
    "    if metric == 'RMSE':\n",
    "        score = metrics.mean_squared_error(labels, preds, squared=False)\n",
    "    elif metric == 'MAE':\n",
    "        score = metrics.mean_absolute_error(labels, preds) \n",
    "    elif metric == 'R2':\n",
    "        score = metrics.r2_score(labels, preds)\n",
    "    elif metric == 'Pearson_R':\n",
    "        score = scipy.stats.pearsonr(labels, preds)[0] \n",
    "    elif metric == 'AUROC':\n",
    "        try:\n",
    "            score = metrics.roc_auc_score(labels, preds) \n",
    "        except ValueError:\n",
    "            score = -1\n",
    "    elif metric == 'AUPRC':\n",
    "        # auc based on precision_recall curve\n",
    "        precision, recall, _ = metrics.precision_recall_curve(labels, preds)\n",
    "        score = metrics.auc(recall, precision)\n",
    "    elif metric == 'Precision_PPV':\n",
    "        # get the roc curve points\n",
    "        false_pos_rate, true_pos_rate, proba = metrics.roc_curve(labels, preds)\n",
    "        # calculate the optimal probability cutoff using Youden's J statistic with equal weight to FP and FN\n",
    "        optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), proba)),\\\n",
    "                                      key=lambda i: i[0], reverse=True)[0][1]\n",
    "        # get hard_preds\n",
    "        hard_preds = [1 if p > optimal_proba_cutoff else 0 for p in preds]\n",
    "        # calculate precision based on hard_preds - pos_label as 1\n",
    "        score = metrics.precision_score(labels, hard_preds, pos_label=1)\n",
    "    elif metric == 'Precision_NPV':\n",
    "        # get the roc curve points\n",
    "        false_pos_rate, true_pos_rate, proba = metrics.roc_curve(labels, preds)\n",
    "        # calculate the optimal probability cutoff using Youden's J statistic with equal weight to FP and FN\n",
    "        optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), proba)),\\\n",
    "                                      key=lambda i: i[0], reverse=True)[0][1]\n",
    "        # get hard_preds\n",
    "        hard_preds = [1 if p > optimal_proba_cutoff else 0 for p in preds]\n",
    "        # calculate precision based on hard_preds - pos_label as 0\n",
    "        score = metrics.precision_score(labels, hard_preds, pos_label=0)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a5b0a",
   "metadata": {},
   "source": [
    "# Calculate performance metrics on all molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f4558",
   "metadata": {},
   "source": [
    "## Benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99694de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the dataset details\n",
    "folder = 'benchmark'\n",
    "task_setting = 'benchmark'\n",
    "mol_props = ['BACE', 'BBBP', 'HIV', 'ESOL', 'FreeSolv',  'Lipop'] \n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14270267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "perf_df = pd.DataFrame(columns=['metric_score', 'metric_name', 'mol_prop', 'model_name','fold', 'split_type', 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d11e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mol_prop in mol_props:\n",
    "    #get the right set of metric_names\n",
    "    if mol_prop in ['BACE', 'BBBP', 'HIV']:\n",
    "        metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "    elif mol_prop in ['ESOL', 'FreeSolv', 'Lipop']:\n",
    "        metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "    for split_type in split_types:\n",
    "        for fold in range(num_folds):\n",
    "            for metric_name in metric_names:\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_rf = pd.read_csv('../results/raw_predictions/RF/{task}/{mol_prop}/{split_type}/r2_b2048_test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #drop NaN values\n",
    "                preds_rf.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_rf['labels'], preds_rf['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'RF', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_molbert = pd.read_csv('../results/raw_predictions/molbert/{task}/{mol_prop}/molbert/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #drop NaN values\n",
    "                preds_molbert.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_molbert['labels'], preds_molbert['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'molbert', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_grover = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #ensure the labels and preds back to double type\n",
    "                preds_grover['labels'] = preds_grover['labels'].astype(float)\n",
    "                preds_grover['preds'] = preds_grover['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover['labels'], preds_grover['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_grover_rdkit = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base_rdkit/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #ensure the labels and preds back to double type\n",
    "                preds_grover_rdkit['labels'] = preds_grover_rdkit['labels'].astype(float)\n",
    "                preds_grover_rdkit['preds'] = preds_grover_rdkit['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover_rdkit.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover_rdkit['labels'], preds_grover_rdkit['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base_rdkit', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fcb921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_score</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>split_type</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>0.168803</td>\n",
       "      <td>Precision_PPV</td>\n",
       "      <td>HIV</td>\n",
       "      <td>RF</td>\n",
       "      <td>0</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0.081454</td>\n",
       "      <td>Precision_PPV</td>\n",
       "      <td>HIV</td>\n",
       "      <td>molbert</td>\n",
       "      <td>0</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>0.036694</td>\n",
       "      <td>Precision_PPV</td>\n",
       "      <td>HIV</td>\n",
       "      <td>grover_base</td>\n",
       "      <td>0</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>0.096070</td>\n",
       "      <td>Precision_PPV</td>\n",
       "      <td>HIV</td>\n",
       "      <td>grover_base_rdkit</td>\n",
       "      <td>0</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>0.313291</td>\n",
       "      <td>Precision_PPV</td>\n",
       "      <td>HIV</td>\n",
       "      <td>RF</td>\n",
       "      <td>1</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric_score    metric_name mol_prop         model_name fold split_type  \\\n",
       "1928      0.168803  Precision_PPV      HIV                 RF    0   scaffold   \n",
       "1929      0.081454  Precision_PPV      HIV            molbert    0   scaffold   \n",
       "1930      0.036694  Precision_PPV      HIV        grover_base    0   scaffold   \n",
       "1931      0.096070  Precision_PPV      HIV  grover_base_rdkit    0   scaffold   \n",
       "1944      0.313291  Precision_PPV      HIV                 RF    1   scaffold   \n",
       "\n",
       "           task  \n",
       "1928  benchmark  \n",
       "1929  benchmark  \n",
       "1930  benchmark  \n",
       "1931  benchmark  \n",
       "1944  benchmark  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df.loc[(perf_df['mol_prop']=='HIV') & (perf_df['metric_name']=='Precision_PPV')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4bcf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to results directory\n",
    "perf_df.to_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae5bf8",
   "metadata": {},
   "source": [
    "## Opioids datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e48115b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the dataset details\n",
    "folder = 'opioids'\n",
    "task_setting = 'cutoff6' # reg, cutoff6\n",
    "mol_props = ['MDR1', 'CYP2D6', 'CYP3A4', 'MOR', 'DOR', 'KOR'] \n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f94d1f",
   "metadata": {},
   "source": [
    "-  prediction over all molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05146cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "perf_df = pd.DataFrame(columns=['metric_score', 'metric_name', 'mol_prop', 'model_name','fold', 'split_type', 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc50abb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mol_prop in mol_props:\n",
    "    #get the right set of metric_names\n",
    "    if task_setting == 'cutoff6':\n",
    "        metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "    elif task_setting == 'reg':\n",
    "        metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "    for split_type in split_types:\n",
    "        for fold in range(num_folds):\n",
    "            for metric_name in metric_names:\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_rf = pd.read_csv('../results/raw_predictions/RF/{task}/{mol_prop}/{split_type}/r2_b2048_test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #drop NaN values\n",
    "                preds_rf.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_rf['labels'], preds_rf['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'RF', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_molbert = pd.read_csv('../results/raw_predictions/molbert/{task}/{mol_prop}/molbert/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #drop NaN values\n",
    "                preds_molbert.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_molbert['labels'], preds_molbert['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'molbert', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_grover = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #convert the labels and preds back to double type\n",
    "                preds_grover['labels'] = preds_grover['labels'].astype(float)\n",
    "                preds_grover['preds'] = preds_grover['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover['labels'], preds_grover['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #colnames: SMILES\tlabels\tpreds\n",
    "                preds_grover_rdkit = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base_rdkit/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(fold=fold, mol_prop=mol_prop, split_type=split_type, task=task_setting))\n",
    "                #convert the labels and preds back to double type\n",
    "                preds_grover_rdkit['labels'] = preds_grover_rdkit['labels'].astype(float)\n",
    "                preds_grover_rdkit['preds'] = preds_grover_rdkit['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover_rdkit.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover_rdkit['labels'], preds_grover_rdkit['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base_rdkit', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                perf_df = perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "799e936f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_score</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>split_type</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [metric_score, metric_name, mol_prop, model_name, fold, split_type, task]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df.loc[(perf_df['mol_prop']=='MDR1') & (perf_df['metric_name']=='RMSE')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e944d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to results directory\n",
    "perf_df.to_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6c29c",
   "metadata": {},
   "source": [
    "# Calculate performance metrics for AC and non-AC molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b859c6",
   "metadata": {},
   "source": [
    "-  Note: AC molecules are those with scaffolds that have pIC50 spanning two magnitudes<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827a6e9",
   "metadata": {},
   "source": [
    "## Opioids datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c48a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify datasets details\n",
    "folder = 'opioids'\n",
    "mol_props = ['MDR1', 'CYP2D6', 'CYP3A4', 'MOR', 'DOR', 'KOR'] \n",
    "task_setting = 'reg' # reg, cutoff6\n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a97e86",
   "metadata": {},
   "source": [
    "-  Note: use **Structure_Analysis.ipynb** to generate the scaffolds_freq.csv and structural_traits.csv for each molecular property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0227f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the AC scaffolds first for each dataset\n",
    "AC_molecules_list = []\n",
    "\n",
    "for mol_prop in mol_props:\n",
    "    #read the scaffolds_freq df\n",
    "    scaffold_freq = pd.read_csv('../results/structures/{folder}/{name}_scaffolds_freq.csv'.format(folder=folder, name=mol_prop))\n",
    "    #get the Murcko-Bemis scaffolds in each dataset\n",
    "    scaffolds = list(scaffold_freq['SMILES'])\n",
    "    \n",
    "    #read the structtraits df\n",
    "    scaff_data = pd.read_csv('../results/structures/{folder}/{name}_structural_traits.csv'.format(folder=folder, name=mol_prop))\n",
    "    #only keep the molecules in scaffolds for AC_scaffolds generation\n",
    "    scaff_data = scaff_data[scaff_data['scaffold'].isin(scaffolds)]\n",
    "    \n",
    "    #make a var to store the AC scaffolds\n",
    "    AC_scaffolds = []\n",
    "    #iterate through all scaffolds and collect the AC_scaffolds\n",
    "    for scaffold in scaffolds:\n",
    "        min_pIC50 = min(scaff_data[scaff_data['scaffold']==scaffold]['label'])\n",
    "        max_pIC50 = max(scaff_data[scaff_data['scaffold']==scaffold]['label'])\n",
    "        \n",
    "        #when the min-max spans at least two, it is an AC scaffold\n",
    "        if max_pIC50 - min_pIC50 >= 2:\n",
    "            AC_scaffolds.append(scaffold)\n",
    "        \n",
    "    #get the molecules with AC scaffolds\n",
    "    AC_molecules = list(scaff_data[scaff_data['scaffold'].isin(AC_scaffolds)]['SMILES'])\n",
    "            \n",
    "    #append the AC_scaffolds to the AC_scaffolds_list\n",
    "    AC_molecules_list.append(AC_molecules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626ecc8",
   "metadata": {},
   "source": [
    "-  for AC molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c7d5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results for AC molecules\n",
    "AC_perf_df = pd.DataFrame(columns=['metric_score', 'metric_name', 'mol_prop', 'model_name','fold', 'split_type', 'task'])\n",
    "\n",
    "for mol_prop in mol_props:\n",
    "    #get the corresponding AC molecules for each mol_prop\n",
    "    AC_molecules = AC_molecules_list[mol_props.index(mol_prop)]\n",
    "    #get the right set of metric_names\n",
    "    if task_setting == 'cutoff6':\n",
    "        metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "    elif task_setting == 'reg':\n",
    "        metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "    for split_type in split_types:\n",
    "        for fold in range(num_folds):\n",
    "            for metric_name in metric_names:\n",
    "                ##read RF raw preds\n",
    "                preds_rf = pd.read_csv('../results/raw_predictions/RF/{task}/{mol_prop}/{split_type}/r2_b2048_test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES in the AC set\n",
    "                preds_rf = preds_rf[preds_rf['SMILES'].isin(AC_molecules)]\n",
    "                #drop NaN values\n",
    "                preds_rf.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_rf['labels'], preds_rf['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'RF', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                AC_perf_df = AC_perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                ##read molbert raw preds\n",
    "                preds_molbert = pd.read_csv('../results/raw_predictions/molbert/{task}/{mol_prop}/molbert/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES in the AC set\n",
    "                preds_molbert = preds_molbert[preds_molbert['SMILES'].isin(AC_molecules)]\n",
    "                #check if the column names of preds_molbert are 'label_targets', 'label_preds'\n",
    "                if list(preds_molbert.columns) == ['label_targets', 'label_preds']:\n",
    "                    #change column names\n",
    "                    preds_molbert.columns = ['labels', 'preds']\n",
    "                #drop NaN values\n",
    "                preds_molbert.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_molbert['labels'], preds_molbert['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'molbert', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                AC_perf_df = AC_perf_df.append(row_to_add, ignore_index=True)\n",
    "                \n",
    "                ##read grover raw preds\n",
    "                preds_grover = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES in the AC set\n",
    "                preds_grover = preds_grover[preds_grover['SMILES'].isin(AC_molecules)]\n",
    "                #convert the labels and preds back to double type\n",
    "                preds_grover['labels'] = preds_grover['labels'].astype(float)\n",
    "                preds_grover['preds'] = preds_grover['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover['labels'], preds_grover['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                AC_perf_df = AC_perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                ##read grover_rdkit raw preds\n",
    "                preds_grover_rdkit = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base_rdkit/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES in the AC set\n",
    "                preds_grover_rdkit = preds_grover_rdkit[preds_grover_rdkit['SMILES'].isin(AC_molecules)]\n",
    "                #convert the labels and preds back to double type\n",
    "                preds_grover_rdkit['labels'] = preds_grover_rdkit['labels'].astype(float)\n",
    "                preds_grover_rdkit['preds'] = preds_grover_rdkit['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover_rdkit.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover_rdkit['labels'], preds_grover_rdkit['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base_rdkit', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                AC_perf_df = AC_perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b39ed",
   "metadata": {},
   "source": [
    "-  for non-AC molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9b58321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results for non-AC molecules\n",
    "nonAC_perf_df = pd.DataFrame(columns=['metric_score', 'metric_name', 'mol_prop', 'model_name','fold', 'split_type', 'suffix'])\n",
    "\n",
    "for mol_prop in mol_props:\n",
    "    #get the corresponding AC molecules for each mol_prop\n",
    "    AC_molecules = AC_molecules_list[mol_props.index(mol_prop)]\n",
    "    #get the right set of metric_names\n",
    "    if task_setting == 'cutoff6':\n",
    "        metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "    elif task_setting == 'reg':\n",
    "        metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "    for split_type in split_types:\n",
    "        for fold in range(num_folds):\n",
    "            for metric_name in metric_names:\n",
    "                ##read RF raw preds\n",
    "                preds_rf = pd.read_csv('../results/raw_predictions/RF/{task}/{mol_prop}/{split_type}/r2_b2048_test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES not in the AC set\n",
    "                preds_rf = preds_rf[~preds_rf['SMILES'].isin(AC_molecules)]\n",
    "                #drop NaN values\n",
    "                preds_rf.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_rf['labels'], preds_rf['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'RF', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                nonAC_perf_df = nonAC_perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                ##read molbert raw preds\n",
    "                preds_molbert = pd.read_csv('../results/raw_predictions/molbert/{task}/{mol_prop}/molbert/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES not in the AC set\n",
    "                preds_molbert = preds_molbert[~preds_molbert['SMILES'].isin(AC_molecules)]\n",
    "                #check if the column names of preds_molbert are 'label_targets', 'label_preds'\n",
    "                if list(preds_molbert.columns) == ['label_targets', 'label_preds']:\n",
    "                    #change column names\n",
    "                    preds_molbert.columns = ['labels', 'preds']\n",
    "                #drop NaN values\n",
    "                preds_molbert.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_molbert['labels'], preds_molbert['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'molbert', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                nonAC_perf_df = nonAC_perf_df.append(row_to_add, ignore_index=True)\n",
    "                \n",
    "                ##read grover raw preds\n",
    "                preds_grover = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES not in the AC set\n",
    "                preds_grover = preds_grover[~preds_grover['SMILES'].isin(AC_molecules)]\n",
    "                #convert the labels and preds back to double type\n",
    "                preds_grover['labels'] = preds_grover['labels'].astype(float)\n",
    "                preds_grover['preds'] = preds_grover['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover['labels'], preds_grover['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                nonAC_perf_df = nonAC_perf_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                ##read grover_rdkit raw preds\n",
    "                preds_grover_rdkit = pd.read_csv('../results/raw_predictions/grover/{task}/{mol_prop}/grover_base_rdkit/{split_type}/test_result_fold{fold}.csv'\\\n",
    "                                          .format(task=task_setting, mol_prop=mol_prop, split_type=split_type, fold=fold))\n",
    "                #only keep the SMILES not in the AC set\n",
    "                preds_grover_rdkit = preds_grover_rdkit[~preds_grover_rdkit['SMILES'].isin(AC_molecules)]\n",
    "                #convert the labels and preds back to double type\n",
    "                preds_grover_rdkit['labels'] = preds_grover_rdkit['labels'].astype(float)\n",
    "                preds_grover_rdkit['preds'] = preds_grover_rdkit['preds'].astype(float)\n",
    "                #drop NaN values\n",
    "                preds_grover_rdkit.dropna(inplace=True)\n",
    "                #calculate metric score\n",
    "                score = metric_calc(preds_grover_rdkit['labels'], preds_grover_rdkit['preds'], metric_name)\n",
    "                #assemble values to add\n",
    "                values_to_add = {'metric_score': score, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                                 'model_name': 'grover_base_rdkit', 'fold': fold, 'split_type': split_type, 'task': task_setting}\n",
    "                #convert to a row series\n",
    "                row_to_add = pd.Series(values_to_add)\n",
    "                #append new row\n",
    "                nonAC_perf_df = nonAC_perf_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79713c5",
   "metadata": {},
   "source": [
    "-  combine the results for AC and non-AC molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9234a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the AC_label\n",
    "AC_perf_df['AC_label'] = 'AC'\n",
    "nonAC_perf_df['AC_label'] = 'non-AC'\n",
    "\n",
    "#combine the AC and nonAC perf_df\n",
    "combined_perf_df = pd.concat([AC_perf_df, nonAC_perf_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b97c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a new grand_perf_df\n",
    "combined_perf_df.to_csv('../results/processed_performance/AC_{folder}_grand_perf_df_{task}.csv'\\\n",
    "                        .format(folder=folder, task=task_setting), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
