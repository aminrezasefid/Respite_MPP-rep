{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b453978f",
   "metadata": {},
   "source": [
    "This notebook is used to conduct pairwise statistical tests on prediction performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8181cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f233340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793ba5a",
   "metadata": {},
   "source": [
    "# Pairwise statistical tests on 30-fold prediction performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b2c78",
   "metadata": {},
   "source": [
    "## Benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0258ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the dataset details\n",
    "folder = 'benchmark'\n",
    "task_setting = 'benchmark'\n",
    "mol_props = ['BACE', 'BBBP', 'HIV', 'ESOL', 'FreeSolv',  'Lipop'] \n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30\n",
    "\n",
    "#specify model names\n",
    "model_names = ['RF', 'molbert', 'grover_base', 'grover_base_rdkit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79912628",
   "metadata": {},
   "source": [
    "-  comparison between model pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527df689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "stats_df = pd.DataFrame(columns=['model_1','model_2', 'metric_name', 'mol_prop', 'split_type', 'task',\\\n",
    "                                 'p_value', 'stats_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b9501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the saved grand_perf_df\n",
    "grand_perf_df = pd.read_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "\n",
    "for split_type in split_types:\n",
    "    for mol_prop in mol_props:\n",
    "        if mol_prop in ['BACE', 'BBBP', 'HIV']:\n",
    "            #cls\n",
    "            metric_names = [\"AUROC\", \"AUPRC\", \"Precision_PPV\", \"Precision_NPV\"]\n",
    "        elif mol_prop in ['ESOL', 'FreeSolv', 'Lipop']:\n",
    "            #reg\n",
    "            metric_names = ['RMSE', 'R2', 'Pearson_R', 'MAE']\n",
    "        \n",
    "        # loop through all metrics\n",
    "        for metric_name in metric_names:\n",
    "            #select the perf_df\n",
    "            perf_df = grand_perf_df.loc[(grand_perf_df['split_type']==split_type) & \\\n",
    "                                        (grand_perf_df['mol_prop']==mol_prop) & \\\n",
    "                                        (grand_perf_df['metric_name']==metric_name)]\n",
    "            #loop through the combinations of the models\n",
    "            for i in range(len(model_names)):\n",
    "                model_1 = model_names[i]\n",
    "                for j in range(len(model_names)):\n",
    "                    model_2 = model_names[j]\n",
    "                    x1 = perf_df.loc[perf_df['model_name']==model_1]['metric_score']\n",
    "                    x2 = perf_df.loc[perf_df['model_name']==model_2]['metric_score']\n",
    "                    \n",
    "                    #calculate the p value using paired t test\n",
    "                    _, p_ttest_rel = stats.ttest_rel(list(x1), list(x2), alternative='two-sided')\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_ttest_rel, 'stats_method': 'ttest_rel'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                    #calculate the p value using Wilcoxon signed-rank test\n",
    "                    try:\n",
    "                        _, p_wilcoxon = stats.wilcoxon(list(x1), list(x2), alternative='two-sided')\n",
    "                    except ValueError:\n",
    "                        p_wilcoxon = 1\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_wilcoxon, 'stats_method': 'wilcoxon'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                    #calculate the p value using independent t test\n",
    "                    _, p_ttest_ind = stats.ttest_ind(list(x1), list(x2), alternative='two-sided')\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_ttest_ind, 'stats_method': 'ttest_ind'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                    #calculate the p value using ranksums test\n",
    "                    _, p_ranksums = stats.ranksums(list(x1), list(x2), alternative='two-sided')\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_ranksums, 'stats_method': 'ranksums'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227dca51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>split_type</th>\n",
       "      <th>task</th>\n",
       "      <th>p_value</th>\n",
       "      <th>stats_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>BACE</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ttest_rel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>BACE</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>1</td>\n",
       "      <td>wilcoxon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>BACE</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ttest_ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>BACE</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ranksums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF</td>\n",
       "      <td>molbert</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>BACE</td>\n",
       "      <td>scaffold</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ttest_rel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_1  model_2 metric_name mol_prop split_type       task p_value  \\\n",
       "0      RF       RF       AUROC     BACE   scaffold  benchmark     NaN   \n",
       "1      RF       RF       AUROC     BACE   scaffold  benchmark       1   \n",
       "2      RF       RF       AUROC     BACE   scaffold  benchmark     1.0   \n",
       "3      RF       RF       AUROC     BACE   scaffold  benchmark     1.0   \n",
       "4      RF  molbert       AUROC     BACE   scaffold  benchmark     0.0   \n",
       "\n",
       "  stats_method  \n",
       "0    ttest_rel  \n",
       "1     wilcoxon  \n",
       "2    ttest_ind  \n",
       "3     ranksums  \n",
       "4    ttest_rel  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e4f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to results/stats\n",
    "stats_df.to_csv('../results/stats/{folder}_stats_df_{task}.csv'.format(folder=folder, task=task_setting), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9a298",
   "metadata": {},
   "source": [
    "## Opioids datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f9c2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the dataset details\n",
    "folder = 'opioids'\n",
    "task_setting = 'reg' # cutoff6, reg\n",
    "mol_props = ['MDR1', 'CYP3A4', 'CYP2D6', 'MOR', 'DOR', 'KOR'] \n",
    "split_types = ['scaffold', 'random']\n",
    "num_folds = 30\n",
    "\n",
    "#specify model names\n",
    "model_names = ['RF', 'molbert', 'grover_base', 'grover_base_rdkit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0cc860",
   "metadata": {},
   "source": [
    "-  comparison between model pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72d7e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "stats_df = pd.DataFrame(columns=['model_1','model_2', 'metric_name', 'mol_prop', 'split_type', 'task',\\\n",
    "                                 'p_value', 'stats_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a709f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the saved grand_perf_df\n",
    "grand_perf_df = pd.read_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "\n",
    "for split_type in split_types:\n",
    "    for mol_prop in mol_props:\n",
    "        # get metric names\n",
    "        if task_setting == 'cutoff6':\n",
    "            metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "        elif task_setting == 'reg':\n",
    "            metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "        \n",
    "        # loop through all metrics\n",
    "        for metric_name in metric_names:\n",
    "            #select the perf_df\n",
    "            perf_df = grand_perf_df.loc[(grand_perf_df['split_type']==split_type) & \\\n",
    "                                        (grand_perf_df['mol_prop']==mol_prop) & \\\n",
    "                                        (grand_perf_df['metric_name']==metric_name)]\n",
    "            #loop through the combinations of the models\n",
    "            for i in range(len(model_names)):\n",
    "                model_1 = model_names[i]\n",
    "                for j in range(len(model_names)):\n",
    "                    model_2 = model_names[j]\n",
    "                    x1 = perf_df.loc[perf_df['model_name']==model_1]['metric_score']\n",
    "                    x2 = perf_df.loc[perf_df['model_name']==model_2]['metric_score']\n",
    "                    \n",
    "                    #calculate the p value using paired t test\n",
    "                    try:\n",
    "                        _, p_ttest_rel = stats.ttest_rel(list(x1), list(x2), alternative='two-sided')\n",
    "                    except ValueError:\n",
    "                        p_wilcoxon = None\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_ttest_rel, 'stats_method': 'ttest_rel'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                    #calculate the p value using Wilcoxon signed-rank test\n",
    "                    try:\n",
    "                        _, p_wilcoxon = stats.wilcoxon(list(x1), list(x2), alternative='two-sided')\n",
    "                    except ValueError:\n",
    "                        p_wilcoxon = 1\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_wilcoxon, 'stats_method': 'wilcoxon'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                    #calculate the p value using independent t test\n",
    "                    _, p_ttest_ind = stats.ttest_ind(list(x1), list(x2), alternative='two-sided')\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_ttest_ind, 'stats_method': 'ttest_ind'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                    #calculate the p value using ranksums test\n",
    "                    _, p_ranksums = stats.ranksums(list(x1), list(x2), alternative='two-sided')\n",
    "                    #assemble values to add\n",
    "                    values_to_add = {'model_1': model_1, 'model_2': model_2, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                    'split_type': split_type, 'task': task_setting, 'p_value': p_ranksums, 'stats_method': 'ranksums'}\n",
    "                    #convert to a row series and append new row\n",
    "                    row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f159c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to results/stats\n",
    "stats_df.to_csv('../results/stats/{folder}_stats_df_{task}.csv'.format(folder=folder, task=task_setting), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbab9d",
   "metadata": {},
   "source": [
    "-  comparison between scaffold and random split to examine inter-scaffold generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2f43f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "stats_df = pd.DataFrame(columns=['model', 'metric_name', 'mol_prop', 'task',\\\n",
    "                                 'p_value', 'stats_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05282106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the grand perf df\n",
    "grand_perf_df = pd.read_csv('../results/processed_performance/{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "\n",
    "for model in model_names:\n",
    "    for mol_prop in mol_props:\n",
    "        # get metric names\n",
    "        if task_setting == 'cutoff6':\n",
    "            metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "        elif task_setting == 'reg':\n",
    "            metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "        for metric_name in metric_names:\n",
    "            #select the perf_df\n",
    "            perf_df = grand_perf_df.loc[(grand_perf_df['model_name']==model) & \\\n",
    "                                        (grand_perf_df['mol_prop']==mol_prop) & \\\n",
    "                                        (grand_perf_df['metric_name']==metric_name)]\n",
    "            \n",
    "            #get the perf under different split type respectively\n",
    "            x1 = perf_df.loc[perf_df['split_type']=='scaffold']['metric_score']\n",
    "            x2 = perf_df.loc[perf_df['split_type']=='random']['metric_score']\n",
    "                    \n",
    "            #calculate the p value using paired t test\n",
    "            _, p_ttest_rel = stats.ttest_rel(list(x1), list(x2), alternative='two-sided')\n",
    "            #assemble values to add\n",
    "            values_to_add = {'model': model, 'metric_name': metric_name, 'mol_prop': mol_prop, 'task': task_setting,\\\n",
    "                             'p_value': p_ttest_rel, 'stats_method': 'ttest_rel'}\n",
    "            #convert to a row series and append new row\n",
    "            row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "            #calculate the p value using Wilcoxon signed-rank test\n",
    "            try:\n",
    "                _, p_wilcoxon = stats.wilcoxon(list(x1), list(x2), alternative='two-sided')\n",
    "            except ValueError:\n",
    "                p_wilcoxon = 1\n",
    "            #assemble values to add\n",
    "            values_to_add = {'model': model, 'metric_name': metric_name, 'mol_prop': mol_prop, 'task': task_setting,\\\n",
    "                             'p_value': p_wilcoxon, 'stats_method': 'wilcoxon'}\n",
    "            #convert to a row series and append new row\n",
    "            row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "            #calculate the p value using independent t test\n",
    "            _, p_ttest_ind = stats.ttest_ind(list(x1), list(x2), alternative='two-sided')\n",
    "            #assemble values to add\n",
    "            values_to_add = {'model': model, 'metric_name': metric_name, 'mol_prop': mol_prop, 'task': task_setting,\\\n",
    "                             'p_value': p_ttest_ind, 'stats_method': 'ttest_ind'}\n",
    "            #convert to a row series and append new row\n",
    "            row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "            #calculate the p value using ranksums test\n",
    "            _, p_ranksums = stats.ranksums(list(x1), list(x2), alternative='two-sided')\n",
    "            #assemble values to add\n",
    "            values_to_add = {'model': model, 'metric_name': metric_name, 'mol_prop': mol_prop, 'task': task_setting,\\\n",
    "                             'p_value': p_ranksums, 'stats_method': 'ranksums'}\n",
    "            #convert to a row series and append new row\n",
    "            row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4943d743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>task</th>\n",
       "      <th>p_value</th>\n",
       "      <th>stats_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>reg</td>\n",
       "      <td>3.257433e-07</td>\n",
       "      <td>ttest_rel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>reg</td>\n",
       "      <td>5.751653e-06</td>\n",
       "      <td>wilcoxon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>reg</td>\n",
       "      <td>1.705933e-06</td>\n",
       "      <td>ttest_ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>reg</td>\n",
       "      <td>6.974497e-06</td>\n",
       "      <td>ranksums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF</td>\n",
       "      <td>MAE</td>\n",
       "      <td>MDR1</td>\n",
       "      <td>reg</td>\n",
       "      <td>1.372884e-10</td>\n",
       "      <td>ttest_rel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model metric_name mol_prop task       p_value stats_method\n",
       "0    RF        RMSE     MDR1  reg  3.257433e-07    ttest_rel\n",
       "1    RF        RMSE     MDR1  reg  5.751653e-06     wilcoxon\n",
       "2    RF        RMSE     MDR1  reg  1.705933e-06    ttest_ind\n",
       "3    RF        RMSE     MDR1  reg  6.974497e-06     ranksums\n",
       "4    RF         MAE     MDR1  reg  1.372884e-10    ttest_rel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06b66bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to results/stats\n",
    "stats_df.to_csv('../results/stats/{folder}_stats_df_{task}_interscaffold.csv'\\\n",
    "                .format(folder=folder, task=task_setting), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2059742",
   "metadata": {},
   "source": [
    "-  comparison between AC and non-AC molecules to examine intra-scaffold generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ae6abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an empty dataframe to attach the results\n",
    "stats_df = pd.DataFrame(columns=['split_type', 'metric_name', 'mol_prop', 'model_name', 'task',\\\n",
    "                                 'p_value', 'stats_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72d214f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the grand perf df\n",
    "grand_perf_df = pd.read_csv('../results/processed_performance/AC_{folder}_grand_perf_df_{task}.csv'.format(folder=folder, task=task_setting))\n",
    "\n",
    "for split_type in split_types:\n",
    "    for mol_prop in mol_props:\n",
    "        # get metric names\n",
    "        if task_setting == 'cutoff6':\n",
    "            metric_names =  ['AUROC', 'AUPRC', 'Precision_PPV', 'Precision_NPV']\n",
    "        elif task_setting == 'reg':\n",
    "            metric_names = ['RMSE', 'MAE', 'R2', 'Pearson_R']\n",
    "        for metric_name in metric_names:\n",
    "            for model_name in model_names:\n",
    "                #select the perf_df\n",
    "                perf_df = grand_perf_df.loc[(grand_perf_df['split_type']==split_type) & \\\n",
    "                                            (grand_perf_df['mol_prop']==mol_prop) & \\\n",
    "                                            (grand_perf_df['metric_name']==metric_name) &\\\n",
    "                                            (grand_perf_df['model_name']==model_name)]\n",
    "                #get the perf under different split types\n",
    "                x1 = perf_df.loc[perf_df['AC_label']=='AC']['metric_score']\n",
    "                x2 = perf_df.loc[perf_df['AC_label']=='non-AC']['metric_score']\n",
    "\n",
    "                #calculate the p value using paired t test\n",
    "                try:\n",
    "                    _, p_ttest_rel = stats.ttest_rel(list(x1), list(x2), alternative='two-sided')\n",
    "                except ValueError:\n",
    "                    p_wilcoxon = None\n",
    "                #assemble values to add\n",
    "                values_to_add = {'split_type': split_type, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                'model_name': model_name, 'task': task_setting, 'p_value': p_ttest_rel, 'stats_method': 'ttest_rel'}\n",
    "                #convert to a row series and append new row\n",
    "                row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #calculate the p value using Wilcoxon signed-rank test\n",
    "                try:\n",
    "                    _, p_wilcoxon = stats.wilcoxon(list(x1), list(x2), alternative='two-sided')\n",
    "                except ValueError:\n",
    "                    p_wilcoxon = 1\n",
    "                #assemble values to add\n",
    "                values_to_add = {'split_type': split_type, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                'model_name': model_name, 'task': task_setting, 'p_value': p_wilcoxon, 'stats_method': 'wilcoxon'}\n",
    "                #convert to a row series and append new row\n",
    "                row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #calculate the p value using independent t test\n",
    "                _, p_ttest_ind = stats.ttest_ind(list(x1), list(x2), alternative='two-sided')\n",
    "                #assemble values to add\n",
    "                values_to_add = {'split_type': split_type, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                'model_name': model_name, 'task': task_setting, 'p_value': p_ttest_ind, 'stats_method': 'ttest_ind'}\n",
    "                #convert to a row series and append new row\n",
    "                row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)\n",
    "\n",
    "                #calculate the p value using ranksums test\n",
    "                _, p_ranksums = stats.ranksums(list(x1), list(x2), alternative='two-sided')\n",
    "                #assemble values to add\n",
    "                values_to_add = {'split_type': split_type, 'metric_name': metric_name, 'mol_prop': mol_prop, \\\n",
    "                'model_name': model_name, 'task': task_setting, 'p_value': p_ranksums, 'stats_method': 'ranksums'}\n",
    "                #convert to a row series and append new row\n",
    "                row_to_add = pd.Series(values_to_add); stats_df = stats_df.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10d99085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_type</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mol_prop</th>\n",
       "      <th>model_name</th>\n",
       "      <th>task</th>\n",
       "      <th>p_value</th>\n",
       "      <th>stats_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>scaffold</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MOR</td>\n",
       "      <td>RF</td>\n",
       "      <td>reg</td>\n",
       "      <td>0.007164</td>\n",
       "      <td>ttest_rel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>scaffold</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MOR</td>\n",
       "      <td>RF</td>\n",
       "      <td>reg</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>wilcoxon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>scaffold</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MOR</td>\n",
       "      <td>RF</td>\n",
       "      <td>reg</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>ttest_ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>scaffold</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>MOR</td>\n",
       "      <td>RF</td>\n",
       "      <td>reg</td>\n",
       "      <td>0.049261</td>\n",
       "      <td>ranksums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>scaffold</td>\n",
       "      <td>MAE</td>\n",
       "      <td>MOR</td>\n",
       "      <td>RF</td>\n",
       "      <td>reg</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>ttest_rel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    split_type metric_name mol_prop model_name task   p_value stats_method\n",
       "192   scaffold        RMSE      MOR         RF  reg  0.007164    ttest_rel\n",
       "193   scaffold        RMSE      MOR         RF  reg  0.011748     wilcoxon\n",
       "194   scaffold        RMSE      MOR         RF  reg  0.013259    ttest_ind\n",
       "195   scaffold        RMSE      MOR         RF  reg  0.049261     ranksums\n",
       "208   scaffold         MAE      MOR         RF  reg  0.013186    ttest_rel"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.loc[(stats_df['model_name']=='RF') & (stats_df['mol_prop']=='MOR')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5861fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to results/stats\n",
    "stats_df.to_csv('../results/stats/AC_{folder}_stats_df_{task}_intrascaffold.csv'\\\n",
    "                .format(folder=folder, task=task_setting), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c82a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
